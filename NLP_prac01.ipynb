{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d4b79b-11bf-42f2-bf7b-a78791f35dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting colorama (from click->nltk)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 6.7 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: regex, joblib, colorama, tqdm, click, nltk\n",
      "Successfully installed click-8.1.8 colorama-0.4.6 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'C:\\Users\\uSER\\AppData\\Roaming\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script nltk.exe is installed in 'C:\\Users\\uSER\\AppData\\Roaming\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fcfacd9-5013-4bf6-afcc-a2cdfc1e2820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "Welcome to the I2IT-NLP Page. \n",
      " Good Morning \t\n",
      "\n",
      "Splitting using whitespece into separate tokens:\n",
      "['Welcome', 'to', 'the', 'I2IT-NLP', 'Page.', 'Good', 'Morning']\n"
     ]
    }
   ],
   "source": [
    "# import WhitespaceTokenizer() method from nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "     \n",
    "# Create a reference variable for Class WhitespaceTokenizer\n",
    "wt = WhitespaceTokenizer()\n",
    "\n",
    "# Create a string input\n",
    "text = \"Welcome to the I2IT-NLP Page. \\n Good Morning \\t\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)     \n",
    "# Use tokenize method\n",
    "tokenized_text = wt.tokenize(text)\n",
    "print(\"\\nSplitting using whitespece into separate tokens:\")\n",
    "print(tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "702b58fd-9038-4832-b144-329bf870fa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "Welcome to the I2IT-NLP Page. \n",
      " Good Morning \t\n",
      "\n",
      "Split all punctuation into separate tokens:\n",
      "['Welcome', 'to', 'the', 'I2IT', '-', 'NLP', 'Page', '.', 'Good', 'Morning']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "text = \"Welcome to the I2IT-NLP Page. \\n Good Morning \\t\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "result = WordPunctTokenizer().tokenize(text)\n",
    "print(\"\\nSplit all punctuation into separate tokens:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf65fbb-e119-46df-92e4-3474b1a77f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome', 'to', 'the', 'I2IT-NLP', 'Page.', 'Good', 'Morning']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    " \n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12aef2c9-9d84-4232-9aa7-1695d4b023f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Who', 'is', 'your', 'favourite', 'cryptocurrency', 'influencer', '?', '🗣', '🏆', 'Tag', 'them', 'below', '!', '👇']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenize = TweetTokenizer()\n",
    "sample_tweet = \"Who is your favourite cryptocurrency influencer? 🗣🏆 Tag them below! 👇\"\n",
    "print(tweet_tokenize.tokenize(sample_tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b75e0977-8115-4a35-8022-956886db9d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In', 'a_little', 'or', 'a_little_bit', 'or', 'a_lot', 'in_spite_of']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import MWETokenizer() method from nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "#from nltk.tokenize import  word_tokenize\n",
    "\n",
    "# Create a reference variable for Class MWETokenizer\n",
    "tokenizer = MWETokenizer([('a', 'little'), ('a', 'little', 'bit'), ('a', 'lot')])\n",
    "\n",
    "tokenizer.add_mwe(('in', 'spite', 'of'))\n",
    "tokenizer.tokenize('In a little or a little bit or a lot in spite of'.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad50f4b4-786e-467a-985a-2e218f6f2e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Steven_Spielberg', 'is', 'an', 'American', 'writer', 'producer', 'director']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer = MWETokenizer()\n",
    "tokenizer.add_mwe(('Steven', 'Spielberg'))\n",
    "tokenizer.tokenize('Steven Spielberg is an American writer producer director '.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcdeda8b-4f82-4d11-8512-b0146b2512e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connector\n",
      "connect\n",
      "connect\n",
      "connect\n",
      "connect\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "example_words =[\"connector\",\"connection\",\"connects\",\"connecting\",\"connected\"]\n",
    "         \n",
    "#Next, we can easily stem by doing something like:\n",
    "for w in example_words:\n",
    "  print(ps.stem(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc762d76-ee7b-450c-a3de-05eea90fd717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous ---> generous\n",
      "generate ---> generat\n",
      "generously ---> generous\n",
      "generation ---> generat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer(language='english')\n",
    "words = ['generous','generate','generously','generation']\n",
    "for word in words:\n",
    "    print(word,\"--->\",snowball.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94b17c0e-bc46-4c96-bd58-038b5ff6eb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eats ---> eat\n",
      "eaten ---> eat\n",
      "puts ---> put\n",
      "putting ---> put\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "words = ['eating','eats','eaten','puts','putting']\n",
    "for word in words:\n",
    "    print(word,\"--->\",lancaster.stem(word))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e7fc5e7-085a-4a5a-8291-ebe3690bc226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mass ---> ma\n",
      "was ---> was\n",
      "bee ---> bee\n",
      "computer ---> computr\n",
      "advisable ---> advi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "regexp = RegexpStemmer('ing|s|e|able', min=4)\n",
    "words = ['mass','was','bee','computer','advisable']\n",
    "for word in words:\n",
    "    print(word,\"--->\",regexp.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54e90bb1-fd4d-423c-bae7-6a6549de8ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      Snowball Stemmer    Lancaster Stemmer             Regexp Stemmer                          \n",
      "generous            gener               generous            gen                           gnrou                                   \n",
      "generate            gener               generat             gen                           gnrat                                   \n",
      "generously          gener               generous            gen                           gnrouly                                 \n",
      "generation          gener               generat             gen                           gnration                                \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(language='english')\n",
    "regexp = RegexpStemmer('ing|s|e|able', min=4)\n",
    "word_list = ['generous','generate','generously','generation']\n",
    "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer'))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word)))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e36d0f31-e8c8-46b4-91d6-f81516129b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\uSER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\uSER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\uSER\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "251155c0-f5b6-464e-9a56-17698e52b6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16031534-68e5-4a80-963c-4186861b820a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
